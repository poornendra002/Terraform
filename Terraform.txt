
[Installation on windows:
  1. https://developer.hashicorp.com/terraform/downloads
  2. Use GIT Bash
  3. echo $PATH
  4. Add extracted folder of terraform to any of the above path 
      ex: /c/Users/anuka/bin
      
      terraform fmt   -> to format the syntax of the yaml file automatically.
  
terraform -version

Terraform 
[Terraform is an open-source infrastructure as code software tool created by Harshi corp that allows us to provision and manage infrastructure resources 
through declarative configuration files. It supports multiple cloud providers, including AWS, Azure, and Google Cloud, as well as on-premises and other infrastructure.]

 	
	- Terraform is written in Golang and available on GitHub 
	
	- Terraform code/configuration is written in HCL (HashiCorp Language) of type .tf
	
	- Terraform helps in prediction of changes through plans (terraform plan) and we can know what changes 
	  will be applied before applying it.

	- Terraform always keeps the infrastructure state saved in terraform.tfstate file and every time u update/make changes and 
	  apply , it always checks with the state file (terraform.tfstate) and it predicts the changes need to be applied.
	  
	- Safe to run terraform code many times because terraform only apply the changes for 
	  the first if the chagnes are not there in state. i.e it matches the desired with actual.

	- Terraform scripts can be version controlled means we keep the terraform 
	  scripts in github.
	  
Terraform Provider 
	- providers are the API to interact with public clouds, in our company aws is the 
	  provider that we are using.
	- In a complete terraform project we can have only one provider block.
		
			provider "aws" {
				access_key = ""
				secret_key = ""
				region = ""
			}
		
	- If we wont configure access_key and secret_key key in provider block then terraform 
	  will try to fetch the keys from aws_cli (taht is when we run aws configure) that is using the below environment variables.
			
			1. AWS_ACCESS_KEY_ID =
			2. AWS_SECRET_ACCESS_KEY = 
			
	Terraform will use the specified service account to authenticate and authorize its interactions with the AWS API.
	

	How to use multiple configurations with same provider block ?
	How to use a provider with multiple region ? 
		
	provider aliases
		- we can define multiple configuration for the same provider and select which provider
		  to use in resource and other blocks.
		
		provider "aws" {
			region = "ap-south-1"
		}
		
		provider "aws" {
			alias = "region1"
			region = "us-east-1"
		}

		provider "aws" {
			alias = "region2"
			region = "us-east-2"
		}		
		
		How to use provider alias ?
			providers = {
  					aws = aws.south
  					}
			
			resource "aws_instance" "my_instance" {
				providers = {
  					aws = aws.south
  					}
				ami = ""
				instance_type = "t2.micro"
			}
			
Terraform Registry 
	- Terraform registry is a repository of modules and resources for provisioning and managing the public clouds.
	- Terraform regisrty is also a community based registry maintaine by terraform community. [Not sure if of this statement :which are written by terraform w.r.t public clouds ]
		URL: regisrty.terraform.io
		
	Example: aws - ec2 instance -> aws_instance
				 - s3 bucket    -> aws_s3_bucket 
				 - vpc 		    -> aws_vpc	
				 
Module Registry
	- Maintained by HarshiCorp. We have predefined modules defined for VPC, EKS, security group, S3 bucket etc.
	- It will lead us to github where main.tf, variables.tf and output.tf are written for each module.
				 
Assignment: List all the resource types of aws provider which you know in aws ? 

Terraform init 
	- This command we use to initialize a working directory containing terraform script as 
	  terraform project. 
	- This in the first command that should be executed after writing a new terraform script or cloning Terraform configurations.
        - It is safe to run this command multiple times. 
			
		The initializations steps are: 
			1. Initializing the Backend ....
			2. Initializing provider plugins ...
			3. Initializing the child modules ...
	
terraform plan 
	- Plan will scan all the .tf scripts in the project directory and it manages to determine 
	  the changes to be applied to cloud (means it determines the desired state).
	- Plan will never do any changes to the actual insfrasructure it just gives what desired 
	  state will be applied.
	- the terraform plan is used to create a execution plan which internally performs a refresh 
	  unless user has disabled this refresh.
	
	To save a plan in a file : This file will be in binary format 
		terraform plan -out <file_name>	
		terraform plan -out plan.out	

terraform apply 
	- The terraform apply command is used to apply the planned changes to the actual cloud 
	  insfrasructure. 
	- This will internally run the plan again then with the user confirmation the predicted 
	  changes will be applied.
		
	To apply changes with out user confirmation 
		terraform apply --auto-approve
	
	To run a particular terraform script
		terraform apply <script>.tf <script1>.tf
		
	To apply a plan file                           [The plan file created using terraform plan -out plan.out]
		terraform apply <plan_file>
	
	To apply one resource block only
		terraform apply -target=resourcetype.resource_name
			
tarraform destroy 
	- This is used to destroy the resources which are manged by this current terraform 
	  scripts.
        - Crate a destroy plan

		To destroy one resource
			terraform destroy -target <resource_type>.<resource_name>
			example: terraform destroy -target aws_instance.ec2_ubuntu

terraform validate 
	- This is to validate the syntax of the terraform scripts in a project.
	
https://registry.terraform.io/providers/hashicorp/aws/latest/docs  :use documentation
--------------------------------------------------------------------------------------------------------------------------------------
Terraform State : it is autogenerated file which gives the information about the resources we have provioned for the project using terraform
	Local - we can keep the state file locally in the same machine where we 
		execute the terraform scripts 
	
	Remote state (Backend) 
		Instead of keeping state file in local we can keep it in a remote location 
		such as s3, etcd. 
	terraform show : The terraform show command is used to display the state of your Terraform infrastructure. 
This command outputs the contents of the Terraform state file, which is a JSON file that represents the current state of your 				
Terraform-managed resources.
               		# terraform show <state_file>
[ etcd is a distributed key-value store, which can be used as a database in certain use cases. It is designed for storing configuration data for distributed systems, 
and it can be used as a centralized/ store for storing data that needs to be shared across multiple systems or applications. ]

Terraform variable and outputs 

[ If an input variable and a local variable have the same name, the value of the input variable will take precedence. This means that if you declare an input variable with the same name as a local variable, the value of the input variable will be used instead of the value of the local variable.

However, it's generally a good practice to use different names for input variables and local variables to avoid any confusion or unexpected behavior. ]

	Local variable 
		- A list of local variable can be declared in a terraform script and it scope 
		  is always within that script.
		- Instead of having same value defined multiple times in same script we can 
		  use local variable so that changing of value will be easy.
		  (its called as an argument)
			locals {
				var1 = value1
				var2 = value2
				.
				.
				varn = valuen	
			}

			To access local variable we use local.<variable> keyword

	Input variables 
		If i do not want to hardcode any values and want to pass parameters to the script, i use input variables. 
		to terraform scripts.
		If we don't define default value then at the time of plan / apply terraform 
		will ask for value as user input in console.
	
			variable "<name>" {
				default = <default_value>
			}

	Output Variables 
	     - Output variables are like return values of terraform scripts.
	     - Output of a resource / module can be used as the input to another.
	     - To suppress the values from the console use sensitive = true 		
			output "<name>" {
			     value = aws_instance.instance1.public_ip	 	
			}

				or
			output "<name>" {
			     value = aws_instance.instance1.public_ip
			     sensitive = true
			}
		
Terraform module 
	All the terraform scripts in a folder is called a module 

	Root module 
	- The .tf files in the main working directory is root module where we have .terrform file 	
	  or .tfstate 
	- Where we run terraform init 

	Child module 
	- Terraform scripts defined in a sub-folder inside root module
	- We can segregate the scripts instead of writing everything in the same script.
	- We can reuse the child modules in other projects also.
	- These are always user defined 
	
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Taint 
	- The terraform taint command is used to mark a terraform managed resource as tainted, which means 
	  the resource to be destroyed and recreated in the next apply.
	- Taint will not mark the actual resource in the cloud but it marks in the state file.
	
	To list resource in state 
		terraform state list
	To taint a resource 
		terraform taint <terra_arn>
		terraform taint aws_instance.example
        To remove the taint
		terraform untaint <terra_arn>
	The command to list tainted resources in Terraform is 
		terraform state list -module=module_name | grep "tainted".

	This command will list all the resources that are currently in a "tainted" state for a specific module. 
	Replace "module_name" with the name of the module you want to check.
	
  [	In this example, the "aws_instance.example" resource is being marked for recreation. When you run "terraform apply" next time, Terraform will destroy and   	    recreate this instance, while leaving other resources in your infrastructure untouched.
	
	You can also use terraform taint -module=MODULE_NAME RESOURCE_ADDRESS to taint a resource that is in a module.
        It's important to note that tainting a resource will not destroy the resource immediately, it will happen only when you run terraform apply next time.  ]



Provisioners 
	- Provisioners are used to copy files and directories, run command from terraform source machine to cloud 
	  resources as a part of resource creation.
	- They are similar to user-data scripts.
	- Most Provisioners requires authentication to run command via ssh or WinRM and we need to
	  define the connection.
	  
	 [ In Terraform, a "connection" block is used to configure the connection settings for a specific provider. 
	 It is typically used within a resource or data block and allows you to specify the connection details for the resource or data source you are creating.
	 
	 The "type" field specifies the type of connection, such as "ssh" or "api". 
	 The "host" field specifies the hostname or IP address of the resource or data source. 
	 The "user" and "password" fields are used to specify the username and password for the connection. 
	 The "private_key" field is used to specify the path to a private key file when using an SSH connection.
	 The syntax for a connection block is as follows:
		(connection is a sub-block)
		connection {
		     type = "ssh"
		     user = "root"
		     password = ""
		     host = "<ip>"	
		     private_key = "PRIVATE_KEY"
		}	
	

	file 
	    - The file Provisioner is used to copy file or directories from terraform project to cloud 
	      compute resource.
	    - This provisioner needs a connection to cloud resource.
	    	 	
	remote-exec 
	    - The remote-exec Provisioner is used to run command or script from terraform project to cloud 
	      compute resource.
	    - This provisioner needs a connection to cloud resource.


	1. ec2 instance with sg 	
	2. file provisioner - should find a file /home/ubuntu/text.txt
	3. remote-exec - Docker installed and file /home/ubuntu/remote-exec.txt
	4. local-exec - creates file Conf/ec2_ips.txt (with ec2 ip address)

[ Provisioners in Terraform are used to configure and provision resources after they have been created. 
They can be used to run scripts, copy files, or execute other actions on the created resources.

There are two types of provisioners in Terraform:

Local provisioners: These provisioners run on the machine where Terraform is executed, and are used to configure the resource after it has been created.
Remote provisioners: These provisioners run on the created resource, and are used to configure the resource after it has been created.
Provisioners are defined within a resource block and can be used to perform tasks like installing software, configuring settings, 
and running scripts on the created resource.  
Here's an example of how to use a local provisioner in Terraform:

resource "aws_instance" "example" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  provisioner "local-exec" {
    command = "echo 'Hello, World!' > index.html"
  }
}

In this example, the local-exec provisioner is used to run a command on the local machine after the aws_instance resource has been created.
Here is an example of how to use a remote provisioner in Terraform:

resource "aws_instance" "example" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  provisioner "remote-exec" {
    inline = [
      "sudo apt-get update",
      "sudo apt-get install -y apache2"
    ]
  }
}

In this example, 
the remote-exec provisioner is used to run commands on the created instance after it has been created.

A "file" provisioner in Terraform allows you to upload a local file to a remote machine created by Terraform as part of a provisioning process. 
This can be useful for uploading configuration files, scripts, or other files that are needed for the operation of the machine.

The syntax for a file provisioner block is as follows:

resource "resource_type" "resource_name" {
  ...
  provisioner "file" {
    source      = "path/to/local/file"
    destination = "path/to/remote/file"
    connection {
      type        = "ssh"
      host        = "hostname"
      user        = "username"
      password    = "password"
      private_key = "path/to/private/key"
    }
  }
}

The source field is the path to the local file that you want to upload, and the destination field is the path on the remote machine where the file should be uploaded.

You can also use the connection block to specify the connection details for the file provisioner, like so:
This is useful when the provisioner needs to connect to the remote machine over SSH.
Keep in mind that the file provisioner is executed after the Terraform resource is created, so the machine must be up and running, and the connection details 
must be valid for the file provisioner to work.

It's important to note that provisioners are executed only once, when the resource is created. 
You should use data resources and external data sources to perform actions on an already created resource.

null resource: 

In Terraform, a "null resource" is a special type of resource that allows you to perform arbitrary actions, such as running scripts or making API calls, 
without actually creating or managing any infrastructure. This can be useful when you need to perform actions that are not directly related to 
provisioning or managing infrastructure, such as sending notifications, triggering webhooks, or creating backups.
A null resource is defined using the null_resource block in Terraform configuration. 

It can have provisioners defined, which will be executed when the resource is created, updated or deleted.

resource "null_resource" "example" {
  provisioner "local-exec" {
    command = "echo 'Hello, World!' > index.html"
  }
}

It's important to note that null resources do not create or manage any infrastructure, they only run provisioners. 
Say for example: The requirement is to run provisioner block inside ec2-instance resource block, as we already have ec2-instance module defined I can 
use null_resource block, so that we can run provsioner as sub-block . null-resource means no resource is being created which means no change in 
the cloud configuration.

]


Terraform workspace 
	- workspace are a way of managing different set of statefiles which are independent to 
	  each other.
	- Every configuration should be in a workspace.
	- "default" is the default workspace in terraform 
	
	To list workspace
		terraform workspace list
	
	To create workspace 
		terraform workspace new <workspace_name>

	To switch between workspace
		terraform workspace select <workspace_name>
	
	To delete workspace 
		terraform workspace delete <workspace_name>
		
	To show which workspace u r in currently
		terraform workspace show

	Usage: 
	   -  We can deploy different version of terraform scripts with states associated with it.
	   -  We can maintain different set of infrastructure with same terraform repo with different 
	      state files.
	   -  We can deploy based on environment categorisation.
[ Terraform workspace is like Git branching
In Terraform, a "workspace" is a way to separate and organize different sets of infrastructure resources. 
Each workspace is essentially a separate Terraform state, allowing you to manage multiple environments (e.g. development, staging, production) 
or projects within the same Terraform configuration.
It's important to note that each workspace maintains its own state and configuration, 
so you should be careful when switching between workspaces to avoid accidentally applying changes to the wrong environment or project.
]

data source / data resource 
	- A data source is used to get the information dynamically from cloud.
	- This block is read-only and this will be always executed on every plan and apply.
	- we can query the required information and filter the field we want.
	Q) I have a terraform state file of VPC, i need to extract information from that to my new state file of ec2 instance. how do u
	do it?
	first mention output variable for vpc
	output "vpc_id" {
	value = ${aws_vpc.<name>.id}
	
	Now in ec2 main.tf file
	data "terraform_remote_state""vpc" {
	backend = s3
	
	config {   #
	same data mentioned in ur vpc main.tf file
	bucket=  
	key=
	region=
	}
	}
	
	resources "aws_subnet" "public_subnet" {
	vpc_id = data.terraform_remote_state.vpc.outputs.<name>.id
	
	
	Incase we are extracting infor from same main file
	
	resources "aws_security" "security1" {
	security_id=aws_subnet.pub;ic_subnet.id
	
	Incase we are extracting infor from same module
	module.resource_type.resource_name
	
[ In Terraform, a "data resource" is a way to retrieve information about an existing resource in a provider's API, without making any changes to that resource. 
This information can then be used in Terraform's configuration and used to create, update or delete other resources. 
For example, a data resource can be used to retrieve the IP address of an existing server, and then use that information to 
configure a firewall rule that allows traffic to that server. 
Data resources are defined in the same way as other resources in Terraform, but with the "data" keyword instead of "resource".]

Null resource 		
	- The null resoruce will have the complete lifecycle support but there will be no change in the cloud configurations.
	- We can use triggers to collect the data of some resources.
	- We can use connections to connect to resoruce using provisioners.
	- The triggers argument allows specifying an arbitory set of values only when changed, triiger will update the value.
	
		resoruce "null_resource" "<resource_name>" {
			
		} 

null_data_resource 
	- This block we are not using in our company we use data block instead of this.
	
	data "null_data_source" "values" {
		  inputs = {
			all_server_ids = concat(aws_instance.green.*.id, aws_instance.blue.*.id)
			all_server_ips = concat(aws_instance.green.*.private_ip, aws_instance.blue.*.private_ip)
		  }
	}	

[In Terraform, a "trigger" is a way to specify that a resource should be recreated or updated when a certain condition is met. 
Triggers can be used to ensure that a resource is always in a certain state, or to handle situations where a resource is managed by 
another system outside of Terraform.
Triggers are specified using the "triggers" block within a resource block. The block can contain one or more triggers, each with a unique name.]


terraform settings 
	- used for specifying provider requirements. 
	- we can restrict the provider version.
	- we configure backend in this settings 
	- we use terraform block 
	- we can control the terraform version.
	- default file terraform.tf / versions.tf 	
		
		terraform {
			
		}

tf.vars vs variables.tf 
	
In summary, variables.tf is used to define the variables and their types, while tf.vars is used to assign values to these variables.
By separating the definition and the assignment of the variables, it allows to keep the structure of the code more clear and also make it easier to manage the same infrastructure in different environments. ]

Terraform refresh 
	- Terraform refresh will find the updates done in the cloud resources by comparing the changes with state file.	
	- This won't modify your real remote objects, but it will modify the the Terraform state.


Terraform import  (Importing existing aws resource to terraform)
	- we are using terraforming tool in our company to import the existing configurations. 
	  Install terraforming in RHEL/Centos 
		(https://github.com/dtan4/terraforming)		
			sudo yum install rubygems
			sudo gem install terraforming
[ The terraform import command is used to import existing resources into a Terraform state. This allows you to take existing resources that were created outside of Terraform and manage them with Terraform.The basic syntax for the terraform import command is as follows:
terraform import [options] <resource address> <resource ID>
The resource address is the address of the resource in the Terraform configuration file, and the resource ID is the ID of the existing resource that you want to import.
For example, if you have an existing AWS S3 bucket with the ID "my-bucket", and you want to import that bucket into Terraform, you would run the following command
terraform import aws_s3_bucket.example my-bucket
terraform import <RESOURCE_TYPE>.<NAME> <ID>
terraform import aws_instance.example i-0123456789abcdef0

This would import the "my-bucket" S3 bucket into Terraform, and create a new state file that includes the S3 bucket as a managed resource. From then on, Terraform will recognize the bucket as a managed resource, and you can use Terraform to update, delete, or track changes to the resource.
In summary, terraform import is a feature of Terraform and it requires the terraform tool to be installed, 
it also needs a corresponding configuration file and access to the resources. ]

Q) How do remove the lock ID from backend
By using command line 
aws s3api delete-object --bucket <bucket_name> --key <key_name>.tflock

https://www.youtube.com/watch?v=q3bdXrHb6WM&list=PLE4eb9xKom3sco8n-UrEXwzO82v49wgFl&index=10

subnet_id= element (list,count.index)     #need to define count here. Say count is more than list, then it starts list from 0 again in round robin fashion. 
count.index starts from 0 , 1 , 2...



Terraform loops (count and for_each)	
	resource "null_resource" "loop_simple" {
		count = 2 
	}
	
	output "loop_out" {
		value = null_resource.loop_simple
	}
	
	
	List count example
	
	locals {
		names = ["bucket1","bucket2","bucket3","bucket4"]
	}
	
	resource "null_resource" "names" {
		count = length(local.names)
		triggers = {
			name = local.names[count.index]
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}
	
	
	List for_each example
	
	locals {
		names = ["bucket1","bucket2","bucket3","bucket4"]
	}
	
	resource "null_resource" "names" {
		for_each = toset(local.names)
		triggers = {
			name = each.value
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}
	
	Map for_each example
	
	locals {
		names = { 
			bucket1 = "region1"
			bucket2 = "region2"
			bucket3 = "region3"
			bucket4 = "region4"
		}
	}
	
	resoruce "null_resource" "names" {
		for_each = local.names
		triggers = {
			bucket = each.key
			region = each.value
		}
	}
	
	output "list_out" {
		value = null_resource.names
	}


Study Terragrunt by Gruntwork.io

Terragrunt is a tool that provides additional functionality on top of Terraform. It is used to manage Terraform modules, and to make it easier to collaborate on Terraform projects. Specifically, it allows for remote state management, automatic locking and dependency management, making it easier to manage and organize Terraform codebase. Additionally, it provides some features like "dry-run" which allows to validate the syntax and the execution plan before applying it.

Terrafrom blocks that i have written
	providers
	local
	variable
	output
	resource 
	attribute
	data
	module
	backend
	terraform
	required_providers
	
Q) U have created a resource using terraform. Now u do not want to manage it using terraform. How do u remove it?
terraform state rm <resource_type>.<resource_name>

Q) You are writing a terraform script to create ec2 instance, you came across a situation that you need to handle some conditions in terraform code. How will you handle these conditions?
Ex: if env is QA, use particular ami, if env is PROD, use another ami and so on.. How will you handle this?


var.a = "" ? var.a : "default-a"

ami = var.environment == "production" ? "ami-xyz123" : "ami-abc123"

Q) You have created terraform file, when you apply it will create an ec2 instance with some EBS volume 100 GB. later you edited it to 200 gb and applied terraform plan again. what will be happening to existing resources?
# aws_instance.example will be updated in-place
  
  ~ resource "aws_instance" "example" {          #   ~ The symbol indicates that the resource will be updated
        # ... other configuration options ...

        root_block_device {
            volume_size = 100 -> 200             #   while the -> symbol indicates the change that will be made.
        }
    }

# aws_ebs_volume.example will be updated in-place
  ~ resource "aws_ebs_volume" "example" {
        # ... other configuration options ...

        size = 100 -> 200
    }
    
    This output indicates that the aws_instance.example resource and the aws_ebs_volume.example resource will both be updated in-place. 
    If there were any other resources that were affected by the change in the Terraform configuration, they would be listed in the plan output as well, with similar indications of whether they will be created, updated, or destroyed.
    
    Q) 




